{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0228f3cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import time\n",
    "import datetime\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.data import random_split\n",
    "from model import Net\n",
    "from sklearn.model_selection import train_test_split\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "d13ce530",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(85644, 64, 60)\n",
      "(85644, 8)\n"
     ]
    }
   ],
   "source": [
    "IMAGE_WIDTH = {5: 15, 20: 60, 60: 180}\n",
    "IMAGE_HEIGHT = {5: 32, 20: 64, 60: 96}  \n",
    "\n",
    "year_list = np.arange(1993,2001,1)\n",
    "\n",
    "images = []\n",
    "label_df = []\n",
    "for year in [1993]:\n",
    "    images.append(np.memmap(os.path.join(\"./img_data/monthly_20d\", f\"20d_month_has_vb_[20]_ma_{year}_images.dat\"), dtype=np.uint8, mode='r').reshape(\n",
    "                        (-1, IMAGE_HEIGHT[20], IMAGE_WIDTH[20])))\n",
    "    label_df.append(pd.read_feather(os.path.join(\"./img_data/monthly_20d\", f\"20d_month_has_vb_[20]_ma_{year}_labels_w_delay.feather\")))\n",
    "    \n",
    "images = np.concatenate(images)\n",
    "label_df = pd.concat(label_df)\n",
    "\n",
    "print(images.shape)\n",
    "print(label_df.shape) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a27437be",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyDataset(Dataset):\n",
    "    \n",
    "    def __init__(self, img, label):\n",
    "        self.img = torch.Tensor(img.copy())\n",
    "        self.label = torch.Tensor(label)\n",
    "        self.len = len(img)\n",
    "  \n",
    "    def __len__(self):\n",
    "        return self.len\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.img[idx], self.label[idx]\n",
    "    \n",
    "    \n",
    "# Use 70%/30% ratio for train/validation split\n",
    "train_indices, val_indices = train_test_split(\n",
    "    np.arange(images.shape[0]),\n",
    "    test_size=0.3,\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "# Create datasets based on the selected indices\n",
    "train_dataset = MyDataset(images[train_indices], (label_df.Ret_5d > 0).values[train_indices])\n",
    "val_dataset = MyDataset(images[val_indices], (label_df.Ret_5d > 0).values[val_indices])\n",
    "\n",
    "# Create data loaders\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=1, shuffle=True, pin_memory=True)\n",
    "val_dataloader = DataLoader(val_dataset, batch_size=1, shuffle=False, pin_memory=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "565c59eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CNN20d(nn.Module):\n",
    "    # Input: [N, (1), 64, 60]; Output: [N, 2]\n",
    "    # Three Convolution Blocks\n",
    "    \n",
    "    def init_weights(self, m):\n",
    "        if isinstance(m, (nn.Linear, nn.Conv2d)):\n",
    "            torch.nn.init.xavier_uniform_(m.weight)\n",
    "            m.bias.data.fill_(0.01)\n",
    "\n",
    "    \n",
    "    def __init__(self):\n",
    "        super(CNN20d, self).__init__()\n",
    "        self.conv1 = nn.Sequential(OrderedDict([\n",
    "            ('Conv', nn.Conv2d(1, 64, (5, 3), padding=(3, 1), stride=(3, 1), dilation=(2, 1))), # output size: [N, 64, 21, 60]\n",
    "            ('BN', nn.BatchNorm2d(64, affine=True)),\n",
    "            ('ReLU', nn.ReLU()),\n",
    "            ('Max-Pool', nn.MaxPool2d((2,1))) # output size: [N, 64, 10, 60]\n",
    "        ]))\n",
    "        self.conv1 = self.conv1.apply(self.init_weights)\n",
    "        \n",
    "        self.conv2 = nn.Sequential(OrderedDict([\n",
    "            ('Conv', nn.Conv2d(64, 128, (5, 3), padding=(3, 1), stride=(1, 1), dilation=(1, 1))), # output size: [N, 128, 12, 60]\n",
    "            ('BN', nn.BatchNorm2d(128, affine=True)),\n",
    "            ('ReLU', nn.ReLU()),\n",
    "            ('Max-Pool', nn.MaxPool2d((2,1))) # output size: [N, 128, 6, 60]\n",
    "        ]))\n",
    "        self.conv2 = self.conv2.apply(self.init_weights)\n",
    "        \n",
    "        self.conv3 = nn.Sequential(OrderedDict([\n",
    "            ('Conv', nn.Conv2d(128, 256, (5, 3), padding=(2, 1), stride=(1, 1), dilation=(1, 1))), # output size: [N, 256, 6, 60]\n",
    "            ('BN', nn.BatchNorm2d(256, affine=True)),\n",
    "            ('ReLU', nn.ReLU()),\n",
    "            ('Max-Pool', nn.MaxPool2d((2,1))) # output size: [N, 256, 3, 60]\n",
    "        ]))\n",
    "        self.conv3 = self.conv3.apply(self.init_weights)\n",
    "\n",
    "        self.DropOut = nn.Dropout(p=0.5)\n",
    "        self.FC = nn.Linear(46080, 2)\n",
    "        self.init_weights(self.FC)\n",
    "        self.Softmax = nn.Softmax(dim=1)\n",
    "\n",
    "    def forward(self, x): # input: [N, 64, 60]\n",
    "        x = x.unsqueeze(1).to(torch.float32)   # output size: [N, 1, 64, 60]\n",
    "        x = self.conv1(x) # output size: [N, 64, 10, 60]\n",
    "        x = self.conv2(x) # output size: [N, 128, 6, 60]\n",
    "        x = self.conv3(x) # output size: [N, 256, 3, 60]\n",
    "        x = self.DropOut(x.view(x.shape[0], -1))\n",
    "        x = self.FC(x) # output size: [N, 2]\n",
    "        x = self.Softmax(x)\n",
    "\n",
    "        \n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "id": "b7799a42",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "\n",
    "def train_n_epochs(n_epochs, model, label_type, train_loader, valid_loader, criterion, optimizer, savefile, early_stop_epoch):\n",
    "    valid_loss_min = np.Inf  # track change in validation loss\n",
    "    train_loss_set = []\n",
    "    valid_loss_set = []\n",
    "    train_acc_set = []\n",
    "    valid_acc_set = []\n",
    "    invariant_epochs = 0\n",
    "\n",
    "    for epoch_i in range(1, n_epochs + 1):\n",
    "\n",
    "        # keep track of training and validation loss\n",
    "        train_loss, train_acc = 0.0, 0.0\n",
    "        valid_loss, valid_acc = 0.0, 0.0\n",
    "        running_loss = 0.0\n",
    "        current = 0\n",
    "\n",
    "        #### Model for training\n",
    "        model.train()\n",
    "        with tqdm(enumerate(train_loader), total=len(train_loader), desc=f'Epoch {epoch_i}, Training') as t:\n",
    "            for i, (data, ret5) in t:\n",
    "                assert label_type in ['RET5', 'RET20'], f\"Wrong Label Type: {label_type}\"\n",
    "                if label_type == 'RET5':\n",
    "                    target = ret5\n",
    "                else:\n",
    "                    target = ret20\n",
    "\n",
    "                if target == 1:\n",
    "                    target = torch.tensor([0, 1]).unsqueeze(0)\n",
    "                    target = target.to(torch.float32)\n",
    "                else:\n",
    "                    target = torch.tensor([1, 0]).unsqueeze(0)\n",
    "                    target = target.to(torch.float32)\n",
    "\n",
    "                data, target = data.to(device), target.to(device)\n",
    "                # clear the gradients of all optimized variables\n",
    "                optimizer.zero_grad()\n",
    "                # forward pass: compute predicted outputs by passing inputs to the model\n",
    "                output = model(data)\n",
    "                # calculate the batch loss\n",
    "                loss = criterion(output, target)\n",
    "                # backward pass: compute gradient of the loss with respect to model parameters\n",
    "                loss.backward()\n",
    "                # perform a single optimization step (parameter update)\n",
    "                optimizer.step()\n",
    "                # update training loss\n",
    "                running_loss = (len(data) * loss.item() + running_loss * current) / (len(data) + current)\n",
    "                current += len(data)\n",
    "                train_loss += loss.item() * data.size(0)\n",
    "                # update training acc\n",
    "                train_acc += (output.argmax(1) == target.argmax(1)).sum()\n",
    "\n",
    "                t.set_postfix({'loss': running_loss})\n",
    "\n",
    "        #### Model for validation\n",
    "        model.eval()\n",
    "        with tqdm(enumerate(valid_loader), total=len(valid_loader), desc=f'Epoch {epoch_i}, Validation') as t:\n",
    "            for i, (data, ret5) in t:\n",
    "                assert label_type in ['RET5', 'RET20'], f\"Wrong Label Type: {label_type}\"\n",
    "                if label_type == 'RET5':\n",
    "                    target = ret5\n",
    "                else:\n",
    "                    target = ret20\n",
    "\n",
    "                if target == 1:\n",
    "                    target = torch.tensor([0, 1]).unsqueeze(0)\n",
    "                    target = target.to(torch.float32)\n",
    "                else:\n",
    "                    target = torch.tensor([1, 0]).unsqueeze(0)\n",
    "                    target = target.to(torch.float32)\n",
    "\n",
    "                # move tensors to GPU if CUDA is available\n",
    "                data, target = data.to(device), target.to(device)\n",
    "                # forward pass: compute predicted outputs by passing inputs to the model\n",
    "                output = model(data)\n",
    "                # calculate the batch loss\n",
    "                loss = criterion(output, target)\n",
    "                # update average validation loss\n",
    "                valid_loss += loss.item() * data.size(0)\n",
    "                valid_acc += (output.argmax(1) == target.argmax(1)).sum()\n",
    "\n",
    "                t.set_postfix({'loss': running_loss})\n",
    "\n",
    "        # Compute average loss\n",
    "        train_loss = train_loss / len(train_loader.sampler)\n",
    "        train_loss_set.append(train_loss)\n",
    "        valid_loss = valid_loss / len(valid_loader.sampler)\n",
    "        valid_loss_set.append(valid_loss)\n",
    "\n",
    "        train_acc = train_acc / len(train_loader.sampler)\n",
    "        train_acc_set.append(train_acc.cpu().numpy())\n",
    "        valid_acc = valid_acc / len(valid_loader.sampler)\n",
    "        valid_acc_set.append(valid_acc.cpu().numpy())\n",
    "\n",
    "        print('Epoch: {} Training Loss: {:.6f} Validation Loss: {:.6f} Training Acc: {:.5f} Validation Acc: {:.5f}'.format(\n",
    "            epoch_i, train_loss, valid_loss, train_acc, valid_acc))\n",
    "\n",
    "        # if validation loss gets smaller, save the model\n",
    "        if valid_loss <= valid_loss_min:\n",
    "            print('Validation loss decreased ({:.6f} --> {:.6f}).  Saving model ...'.format(valid_loss_min, valid_loss))\n",
    "            valid_loss_min = valid_loss\n",
    "            invariant_epochs = 0\n",
    "            torch.save({\n",
    "                'epoch': epoch_i,\n",
    "                'model_state_dict': model.state_dict(),\n",
    "                'optimizer_state_dict': optimizer.state_dict()\n",
    "            }, savefile)\n",
    "        else:\n",
    "            invariant_epochs = invariant_epochs + 1\n",
    "\n",
    "        if invariant_epochs >= early_stop_epoch:\n",
    "            print(f\"Early Stop at Epoch [{epoch_i}]: Performance hasn't enhanced for {early_stop_epoch} epochs\")\n",
    "            break\n",
    "\n",
    "    return train_loss_set, valid_loss_set, train_acc_set, valid_acc_set\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "id": "c2111387",
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1, Training:   0%|     | 2214/555113 [01:00<4:12:35, 36.48it/s, loss=1.15]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[135], line 14\u001b[0m\n\u001b[1;32m     11\u001b[0m criterion \u001b[38;5;241m=\u001b[39m nn\u001b[38;5;241m.\u001b[39mBCELoss()\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[1;32m     12\u001b[0m optimizer \u001b[38;5;241m=\u001b[39m optim\u001b[38;5;241m.\u001b[39mAdam(model\u001b[38;5;241m.\u001b[39mparameters(), lr\u001b[38;5;241m=\u001b[39mLEARNING_RATE, weight_decay\u001b[38;5;241m=\u001b[39mWEIGHT_DECAY)\n\u001b[0;32m---> 14\u001b[0m \u001b[43mtrain_n_epochs\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m100\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mRET5\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_dataloader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mval_dataloader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcriterion\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcriterion\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m     15\u001b[0m \u001b[43m               \u001b[49m\u001b[43mearly_stop_epoch\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m5\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msavefile\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mMODEL_SAVE_FILE\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[133], line 45\u001b[0m, in \u001b[0;36mtrain_n_epochs\u001b[0;34m(n_epochs, model, label_type, train_loader, valid_loader, criterion, optimizer, savefile, early_stop_epoch)\u001b[0m\n\u001b[1;32m     43\u001b[0m loss \u001b[38;5;241m=\u001b[39m criterion(output, target)\n\u001b[1;32m     44\u001b[0m \u001b[38;5;66;03m# backward pass: compute gradient of the loss with respect to model parameters\u001b[39;00m\n\u001b[0;32m---> 45\u001b[0m \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     46\u001b[0m \u001b[38;5;66;03m# perform a single optimization step (parameter update)\u001b[39;00m\n\u001b[1;32m     47\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mstep()\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/torch/_tensor.py:487\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    477\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    478\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    479\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[1;32m    480\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    485\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[1;32m    486\u001b[0m     )\n\u001b[0;32m--> 487\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    488\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[1;32m    489\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/torch/autograd/__init__.py:200\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    195\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[1;32m    197\u001b[0m \u001b[38;5;66;03m# The reason we repeat same the comment below is that\u001b[39;00m\n\u001b[1;32m    198\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    199\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 200\u001b[0m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[1;32m    201\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    202\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from collections import OrderedDict\n",
    "import torch.optim as optim\n",
    "\n",
    "\n",
    "LEARNING_RATE = .00001\n",
    "WEIGHT_DECAY = 0.01\n",
    "MODEL_SAVE_FILE = 'CNN/models/model.pt'\n",
    "\n",
    "device = 'cpu'\n",
    "model = CNN20d()\n",
    "criterion = nn.BCELoss().to(device)\n",
    "optimizer = optim.Adam(model.parameters(), lr=LEARNING_RATE, weight_decay=WEIGHT_DECAY)\n",
    "\n",
    "train_n_epochs(100, model, 'RET5', train_dataloader, val_dataloader, criterion=criterion, optimizer=optimizer, \n",
    "               early_stop_epoch=5, savefile=MODEL_SAVE_FILE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9a6f8af",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
